from openai import OpenAI
import os
from pathlib import Path
from dotenv import load_dotenv
import copy
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import re


def load_env():
    """è‡ªåŠ¨åŠ è½½ç¯å¢ƒå˜é‡"""
    env_path = Path(__file__).resolve().parent / ".env"
    if not load_dotenv(env_path):
        raise FileNotFoundError("âŒ cannot find .env file")


def create_client() -> OpenAI:
    """åˆ›å»º OpenAI å®¢æˆ·ç«¯"""
    return OpenAI(
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url=os.getenv("DEEPSEEK_API_URL")
        # api_key=os.getenv("OPENAI_API_KEY"),
        # base_url=os.getenv("OPENAI_API_URL"),
    )


def call_openai_chat(client: OpenAI, messages: list) -> str:
    """è°ƒç”¨ OpenAI èŠå¤©æ¨¡å‹"""
    response = client.chat.completions.create(
        model="deepseek-chat",
        # model="deepseek-reasoner",
        # model="gpt-3.5-turbo",
        # model="gpt-4o",
        # model="o3",
        messages=messages,
        temperature=0.7
    )
    return response.choices[0].message.content.strip()


def extract_final_answer(text):
    """
    Extract the final answer from the response, supporting various formats and multi-line extraction
    """
    # Match "Repeat the final answer" or similar prompts followed by content between triple backticks
    match = re.search(r"(?:Repeat the final answer|Final answer|LTL Formula).*?:?\s*```(.*?)```", text, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()

    # Match simple triple backtick format
    match = re.search(r"```(.*?)```", text, re.DOTALL)
    if match:
        return match.group(1).strip()

    return None


def nl2ltl(prompt: str, client: OpenAI) -> str:
    complete_result = ""
    history = []
    messages = [
        {"role": "system", "content": (
            "You are an expert in natural language to Linear Temporal Logic (LTL) conversion. "
            "Please solve the following problem independently and precisely.\n\n"
            "IMPORTANT: Format your response as follows:\n"
            "1. First provide your detailed reasoning and analysis\n"
            "2. End with: Repeat the final answer of the original question: ```\n[your_final_answer]\n```\n"
            "3. Make sure your final answer is enclosed in triple backticks with newlines before and after\n"
            "4. For LTL formulas, use standard notation with correct symbols\n"
            "5. Your final answer between triple backticks must be the exact formula with no additional text"
        )},
        {"role": "user", "content": prompt}
    ]

    reply = call_openai_chat(client, messages)
    history.append({"role": "assistant", "content": reply})

    # Extract the final answer
    final_answer = extract_final_answer(reply)

    print(f"\nğŸ¤– Agent's answer: \n{reply}")
    complete_result += f"\nğŸ¤– Agent's answer: \n{reply}\n"

    if final_answer:
        print(f"\nğŸ“ Extracted final answer: \n{final_answer}")
        complete_result += f"\nğŸ“ Extracted final answer: \n{final_answer}\n"
    else:
        print("\nâš ï¸ Could not extract a clear final answer")
        complete_result += "\nâš ï¸ Could not extract a clear final answer\n"

    return complete_result


def end2end(question: str) -> str:
    load_env()
    client = create_client()
    return nl2ltl(question, client)


# ç¤ºä¾‹è°ƒç”¨
if __name__ == "__main__":
    batch = True  # Set to True for batch processing, False for single question
    if not batch:
        # open the base prompt file
        with open("config/prompts/base_prompt.txt") as f:
            # read the base prompt content
            base_prompt = f.read()  # ä»»åŠ¡æè¿° + è¾“å‡ºæ ¼å¼ + çŸ¥è¯†åº“ + æ ·ä¾‹ + æ‰§è¡Œè¦æ±‚ + é—®é¢˜
        Question = "Globally, if a is true, then b will be true in the next step."
        base_prompt = base_prompt.replace("[INPUT TEXT]", Question)
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼Œæ„æˆå®Œæ•´çš„ Prompt
        # è¯»å– examples.xlsx ä¸­çš„ç¤ºä¾‹ï¼Œä¸è¾“å…¥çš„é—®é¢˜è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—
        # è¯»å–examples.xlsxæ–‡ä»¶ä¸­çš„ç¤ºä¾‹æ•°æ®
        examples_df = pd.read_excel("data/examples.xlsx")

        # åŠ è½½æ¨¡å‹
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

        # ç”Ÿæˆå¥å­åµŒå…¥
        example_texts = examples_df["Input Text"].tolist()
        all_texts = [Question] + example_texts
        embeddings = model.encode(all_texts, convert_to_tensor=True)

        # è®¡ç®—ç›¸ä¼¼åº¦
        cosine_scores = util.pytorch_cos_sim(embeddings[0], embeddings[1:])[0]

        # è·å–å‰5ä¸ªæœ€ç›¸ä¼¼çš„ç´¢å¼•
        top_indices = cosine_scores.argsort(descending=True)[:5].tolist()

        # æ„å»ºç¤ºä¾‹æ¨¡æ¿
        examples_template = ""
        for idx in top_indices:
            input_text = examples_df.iloc[idx]["Input Text"]
            analysis_process = examples_df.iloc[idx]["Analysis Process"]

            examples_template += f"**<Example>**\n"
            examples_template += f"**Input Text**: {input_text}\n"
            examples_template += f"**Analysis Process**:\n{analysis_process}\n\n"

        # å°†ç¤ºä¾‹æ¨¡æ¿æ·»åŠ åˆ°base_promptä¸­
        base_prompt = base_prompt.replace("[Examples]", examples_template)
        print(base_prompt)
        result = end2end(base_prompt)
        print("\nâœ… The final output result:\n", result)
    else:
        # æ‰¹é‡å¤„ç†æµ‹è¯•é›†
        print("\nå¼€å§‹æ‰¹é‡å¤„ç†æ•°æ®é›†...")

        # è¯»å–å¾…å¤„ç†çš„æ•°æ®é›†
        dataset_df = pd.read_excel("data/input/nl2spec-dataset.xlsx")  # nl2spec dataset
        requirements = dataset_df["NL"].tolist()  # NL

        # dataset_df = pd.read_excel("data/input/temp.xlsx")  # dataset
        # requirements = dataset_df["Requirement"].tolist()  # Requirement

        # åˆ›å»ºç»“æœåˆ—è¡¨
        results = []
        result_file = "data/output/end2end/end2end-deepseek-v3-nl2spec-result-checked-new.xlsx"

        # è¯»å–base_promptæ¨¡æ¿
        with open("config/prompts/base_prompt.txt") as f:
            base_prompt = f.read()
        # æ‰¹é‡å¤„ç†æ¯ä¸ªéœ€æ±‚
        for i, req in enumerate(requirements):
            print(f"\nå¤„ç†éœ€æ±‚ {i + 1}/{len(requirements)}: {req[:50]}...")

            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            current_prompt = copy.deepcopy(base_prompt)  # æ›¿æ¢ä¸ºæ·±æ‹·è´ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ¨¡æ¿
            current_prompt = current_prompt.replace("[INPUT TEXT]", req)

            examples_df = pd.read_excel("data/examples.xlsx")
            # åŠ è½½æ¨¡å‹
            model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
            # ç”Ÿæˆå¥å­åµŒå…¥
            example_texts = examples_df["Input Text"].tolist()
            all_texts = [req] + example_texts
            embeddings = model.encode(all_texts, convert_to_tensor=True)

            # è®¡ç®—ç›¸ä¼¼åº¦
            cosine_scores = util.pytorch_cos_sim(embeddings[0], embeddings[1:])[0]

            # è·å–å‰5ä¸ªæœ€ç›¸ä¼¼çš„ç´¢å¼•
            top_indices = cosine_scores.argsort(descending=True)[:5].tolist()

            examples_template = ""
            for idx in top_indices:
                input_text = examples_df.iloc[idx]["Input Text"]
                analysis_process = examples_df.iloc[idx]["Analysis Process"]
                examples_template += f"**<Example>**\n"
                examples_template += f"**Input Text**: {input_text}\n"
                examples_template += f"**Analysis Process**:\n{analysis_process}\n\n"

            current_prompt = current_prompt.replace("[Examples]", examples_template)

            result = end2end(current_prompt)

            # å°†ç»“æœæ·»åŠ åˆ°åˆ—è¡¨
            results.append({"Requirement": req, "Result": result})

            # æ¯5ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœï¼ˆé˜²æ­¢è¿è¡Œä¸­æ–­å¯¼è‡´å…¨éƒ¨ä¸¢å¤±ï¼‰
            if (i + 1) % 5 == 0 or i == len(requirements) - 1:
                pd.DataFrame(results).to_excel(result_file, index=False)
                print(f"å·²ä¿å­˜ {len(results)} ä¸ªç»“æœåˆ° {result_file}")

        print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ! å·²å¤„ç† {len(results)} ä¸ªéœ€æ±‚ï¼Œç»“æœå·²ä¿å­˜åˆ° {result_file}")