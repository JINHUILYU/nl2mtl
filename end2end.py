"""
End-to-End NL2MTL Processing System
Simplified version with configuration-based prompt construction, optional RAG, and single agent processing.

åŠŸèƒ½æµç¨‹ï¼š
1. åŠ è½½é…ç½®æ–‡ä»¶
2. æ ¹æ®é…ç½®æ„é€ promptï¼ˆå¯é€‰RAGï¼‰
3. å‘é€ç»™å•ä¸ªagentå¤„ç†
4. æå–ç»“æœå¹¶ç»Ÿè®¡tokenæ¶ˆè€—
5. ä¿å­˜ç»“æœ
"""

import json
import time
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from pathlib import Path
from sentence_transformers import SentenceTransformer, util
from openai import OpenAI
from dotenv import load_dotenv
import logging
import os
import re
import copy

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TokenUsage:
    """Tokenä½¿ç”¨ç»Ÿè®¡"""
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0

@dataclass
class ProcessResult:
    """å¤„ç†ç»“æœ"""
    input_sentence: str
    final_mtl_expression: Optional[str]
    agent_response: str
    token_usage: TokenUsage
    processing_time: float
    rag_enabled: bool
    examples_used: Optional[str] = None

class End2EndNL2MTL:
    """ç«¯åˆ°ç«¯NL2MTLå¤„ç†å™¨"""
    
    def __init__(self, config_path: str = "config/config.json"):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.config = self._load_config(config_path)
        self.client = self._initialize_client()
        self.sentence_model = None
        self.examples_data = None
        self.base_prompt = self._load_base_prompt()
        
        # è·å–RAGé…ç½®
        self.rag_enabled = self.config.get("rag_enabled", True)
        logger.info(f"RAGå¯ç”¨çŠ¶æ€: {self.rag_enabled}")
        
        # å¦‚æœå¯ç”¨RAGï¼Œåˆå§‹åŒ–ç›¸å…³ç»„ä»¶
        if self.rag_enabled:
            self._initialize_rag_components()
        
        # è·å–agenté…ç½®
        self.agent_config = self.config["agents"][0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªagent
        logger.info(f"ä½¿ç”¨Agent: {self.agent_config['name']} ({self.agent_config['model']})")
        
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "agents": [
                {
                    "name": "Agent_A",
                    "role": "logician",
                    "model": "deepseek-chat",
                    "temperature": 0.7,
                    "api_key_env": "DEEPSEEK_API_KEY",
                    "base_url_env": "DEEPSEEK_API_URL"
                }
            ],
            "rag_enabled": True
        }
    
    def _initialize_client(self) -> OpenAI:
        """åˆå§‹åŒ–APIå®¢æˆ·ç«¯"""
        load_dotenv()
        
        agent_config = self.config["agents"][0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªagent
        api_key = os.getenv(agent_config["api_key_env"])
        base_url = os.getenv(agent_config["base_url_env"])
        
        if not api_key:
            raise ValueError(f"APIå¯†é’¥æœªæ‰¾åˆ°: {agent_config['api_key_env']}")
            
        return OpenAI(
            api_key=api_key,
            base_url=base_url
        )
    
    def _initialize_rag_components(self):
        """åˆå§‹åŒ–RAGç›¸å…³ç»„ä»¶"""
        try:
            self.sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
            self.examples_data = pd.read_excel("data/input/examples.xlsx")
            logger.info("RAGç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"RAGç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            self.rag_enabled = False
    
    def _load_base_prompt(self) -> str:
        """åŠ è½½åŸºç¡€promptæ¨¡æ¿"""
        try:
            with open("config/base_prompt.txt", 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            logger.warning("åŸºç¡€promptæ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤prompt")
            return """è¯·åˆ†æä»¥ä¸‹è‡ªç„¶è¯­è¨€å¥å­å¹¶ç”Ÿæˆå¯¹åº”çš„MTLè¡¨è¾¾å¼ã€‚

å¥å­: [INPUT TEXT]

[EXAMPLE]

è¯·æä¾›è¯¦ç»†çš„åˆ†æè¿‡ç¨‹å’Œæœ€ç»ˆçš„MTLè¡¨è¾¾å¼ã€‚

IMPORTANT: Format your response as follows:
1. First provide your detailed reasoning and analysis
2. End with: Repeat the final answer of the original question: ```
[your_final_answer]
```
3. Make sure your final answer is enclosed in triple backticks with newlines before and after
4. For MTL formulas, use standard notation with correct symbols
5. Your final answer between triple backticks must be the exact formula with no additional text"""
    
    def _calculate_similarity(self, sentence: str, examples: List[str]) -> List[float]:
        """è®¡ç®—å¥å­ä¸ç¤ºä¾‹çš„ç›¸ä¼¼åº¦"""
        if self.sentence_model is None:
            logger.error("å¥å­æ¨¡å‹æœªåˆå§‹åŒ–")
            return [0.0] * len(examples)
            
        try:
            # ç¼–ç è¾“å…¥å¥å­å’Œæ‰€æœ‰ç¤ºä¾‹
            sentence_embedding = self.sentence_model.encode([sentence], convert_to_tensor=True)
            example_embeddings = self.sentence_model.encode(examples, convert_to_tensor=True)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = util.pytorch_cos_sim(sentence_embedding, example_embeddings)[0]
            return similarities.cpu().numpy().tolist()
        except Exception as e:
            logger.error(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return [0.0] * len(examples)
    
    def _get_top_examples(self, sentence: str, top_k: int = 5) -> str:
        """è·å–æœ€ç›¸ä¼¼çš„top-kä¸ªç¤ºä¾‹"""
        # å¦‚æœRAGæœªå¯ç”¨ï¼Œè¿”å›ç©ºç¤ºä¾‹
        if not self.rag_enabled:
            logger.info("RAGæœªå¯ç”¨ï¼Œè·³è¿‡ç¤ºä¾‹æ£€ç´¢")
            return ""
        
        if self.examples_data is None or self.examples_data.empty:
            logger.warning("ç¤ºä¾‹æ•°æ®æœªåŠ è½½æˆ–ä¸ºç©º")
            return ""
        
        try:
            # è·å–ç¤ºä¾‹æ•°æ®çš„åˆ—åï¼ˆå…¼å®¹ä¸åŒçš„åˆ—åï¼‰
            input_col = None
            output_col = None
            
            for col in self.examples_data.columns:
                if any(keyword in col.lower() for keyword in ['input', 'text', 'natural', 'rule']):
                    input_col = col
                elif any(keyword in col.lower() for keyword in ['answer', 'output', 'mtl', 'formula']):
                    output_col = col
            
            if input_col is None or output_col is None:
                logger.error(f"æ— æ³•è¯†åˆ«ç¤ºä¾‹æ•°æ®çš„åˆ—å: {list(self.examples_data.columns)}")
                return ""
            
            examples = self.examples_data[input_col].tolist()
            answers = self.examples_data[output_col].tolist()
            
            similarities = self._calculate_similarity(sentence, examples)
            
            # è·å–top-kç´¢å¼•
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            # æ„å»ºç¤ºä¾‹æ–‡æœ¬
            example_text = ""
            for i, idx in enumerate(top_indices, 1):
                example_text += f"**<Example {i}>**\n"
                example_text += f"**Input Text**: {examples[idx]}\n"
                example_text += f"**Analysis Process**: {answers[idx]}\n\n"
            
            logger.info(f"æ£€ç´¢åˆ° {len(top_indices)} ä¸ªç›¸ä¼¼ç¤ºä¾‹")
            return example_text
            
        except Exception as e:
            logger.error(f"ç¤ºä¾‹æ£€ç´¢å¤±è´¥: {e}")
            return ""
    
    def _call_llm(self, messages: List[Dict[str, str]]) -> tuple[str, TokenUsage]:
        """è°ƒç”¨LLMå¹¶è¿½è¸ªtokenä½¿ç”¨"""
        try:
            response = self.client.chat.completions.create(
                model=self.agent_config["model"],
                messages=messages,  # type: ignore
                temperature=self.agent_config["temperature"]
            )
            
            # è¿½è¸ªtokenä½¿ç”¨
            token_usage = TokenUsage()
            if hasattr(response, 'usage') and response.usage:
                token_usage.prompt_tokens = response.usage.prompt_tokens
                token_usage.completion_tokens = response.usage.completion_tokens
                token_usage.total_tokens = response.usage.total_tokens
            
            content = response.choices[0].message.content
            return content.strip() if content else "", token_usage
            
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return f"LLMè°ƒç”¨å¤±è´¥: {str(e)}", TokenUsage()
    
    def _extract_final_answer(self, text: str) -> Optional[str]:
        """ä»å›ç­”ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
        # åŒ¹é… "Repeat the final answer" æˆ–ç±»ä¼¼æç¤ºåçš„ä¸‰é‡åå¼•å·å†…å®¹
        match = re.search(r"(?:Repeat the final answer|Final answer|MTL Formula).*?:?\s*```(.*?)```", text, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()

        # åŒ¹é…ç®€å•çš„ä¸‰é‡åå¼•å·æ ¼å¼
        match = re.search(r"```(.*?)```", text, re.DOTALL)
        if match:
            return match.group(1).strip()

        # å°è¯•æå–åŒ…å«MTLæ“ä½œç¬¦çš„è¡Œ
        lines = text.split('\n')
        for line in lines:
            if any(op in line for op in ['G(', 'F_[', 'P_[', 'U_[', 'X(', 'G[', 'F[', 'U[', 'P[']):
                return line.strip()

        return None
    
    def process_single(self, sentence: str) -> ProcessResult:
        """å¤„ç†å•ä¸ªå¥å­"""
        start_time = time.time()
        logger.info(f"å¼€å§‹å¤„ç†å¥å­: {sentence}")
        
        try:
            # è·å–ç›¸ä¼¼ç¤ºä¾‹ï¼ˆå¦‚æœå¯ç”¨RAGï¼‰
            examples = self._get_top_examples(sentence) if self.rag_enabled else ""
            
            # æ„å»ºå®Œæ•´çš„prompt
            prompt = self.base_prompt.replace("[INPUT TEXT]", sentence)
            if self.rag_enabled and examples:
                prompt = prompt.replace("[EXAMPLE]", examples)
            else:
                prompt = prompt.replace("[EXAMPLE]", "")
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "system", 
                    "content": "You are an expert in natural language to Linear Temporal Logic (LTL) conversion. Please solve the following problem independently and precisely."
                },
                {"role": "user", "content": prompt}
            ]
            
            # è°ƒç”¨LLM
            response, token_usage = self._call_llm(messages)
            
            # æå–æœ€ç»ˆç­”æ¡ˆ
            final_answer = self._extract_final_answer(response)
            
            processing_time = time.time() - start_time
            
            logger.info(f"å¤„ç†å®Œæˆï¼Œç”¨æ—¶: {processing_time:.2f}ç§’ï¼ŒTokenä½¿ç”¨: {token_usage.total_tokens}")
            
            return ProcessResult(
                input_sentence=sentence,
                final_mtl_expression=final_answer,
                agent_response=response,
                token_usage=token_usage,
                processing_time=processing_time,
                rag_enabled=self.rag_enabled,
                examples_used=examples if self.rag_enabled else None
            )
            
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {e}")
            processing_time = time.time() - start_time
            return ProcessResult(
                input_sentence=sentence,
                final_mtl_expression=None,
                agent_response=f"å¤„ç†å¤±è´¥: {str(e)}",
                token_usage=TokenUsage(),
                processing_time=processing_time,
                rag_enabled=self.rag_enabled
            )
    
    def process_batch(self, sentences: List[str], output_file: str) -> List[ProcessResult]:
        """æ‰¹é‡å¤„ç†å¥å­"""
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(sentences)} ä¸ªå¥å­")
        
        results = []
        total_tokens = 0
        total_time = 0
        
        for i, sentence in enumerate(sentences, 1):
            logger.info(f"å¤„ç†ç¬¬ {i}/{len(sentences)} ä¸ªå¥å­")
            
            result = self.process_single(sentence)
            results.append(result)
            
            total_tokens += result.token_usage.total_tokens
            total_time += result.processing_time
            
            # æ¯5ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if i % 5 == 0 or i == len(sentences):
                self.save_batch_results(results, output_file)
                logger.info(f"å·²ä¿å­˜ {len(results)} ä¸ªç»“æœåˆ° {output_file}")
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼æ€»Tokenä½¿ç”¨: {total_tokens}ï¼Œæ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        return results
    
    def save_result(self, result: ProcessResult, output_file: str):
        """ä¿å­˜å•ä¸ªç»“æœ"""
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)
        
        save_data = {
            "input_sentence": result.input_sentence,
            "final_mtl_expression": result.final_mtl_expression,
            "agent_response": result.agent_response,
            "token_usage": asdict(result.token_usage),
            "processing_time": result.processing_time,
            "rag_enabled": result.rag_enabled,
            "examples_used": result.examples_used,
            "agent_config": {
                "name": self.agent_config["name"],
                "model": self.agent_config["model"],
                "temperature": self.agent_config["temperature"]
            },
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    def save_batch_results(self, results: List[ProcessResult], output_file: str):
        """ä¿å­˜æ‰¹é‡ç»“æœ"""
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)
        
        # è½¬æ¢ä¸ºDataFrameæ ¼å¼ä¾¿äºæŸ¥çœ‹
        batch_data = []
        total_tokens = 0
        total_time = 0
        
        for result in results:
            batch_data.append({
                "Input_Sentence": result.input_sentence,
                "Final_MTL_Expression": result.final_mtl_expression,
                "Agent_Response": result.agent_response,
                "Processing_Time": result.processing_time,
                "Token_Usage": result.token_usage.total_tokens,
                "RAG_Enabled": result.rag_enabled
            })
            total_tokens += result.token_usage.total_tokens
            total_time += result.processing_time
        
        # ä¿å­˜ä¸ºExcelæ–‡ä»¶
        df = pd.DataFrame(batch_data)
        df.to_excel(output_file, index=False)
        
        # åŒæ—¶ä¿å­˜è¯¦ç»†çš„JSONç»“æœ
        json_file = output_file.replace('.xlsx', '_detailed.json')
        detailed_data = {
            "summary": {
                "total_sentences": len(results),
                "total_tokens": total_tokens,
                "total_time": total_time,
                "average_time_per_sentence": total_time / len(results) if results else 0,
                "agent_config": {
                    "name": self.agent_config["name"],
                    "model": self.agent_config["model"],
                    "temperature": self.agent_config["temperature"]
                },
                "rag_enabled": self.rag_enabled,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            },
            "results": [
                {
                    "input_sentence": result.input_sentence,
                    "final_mtl_expression": result.final_mtl_expression,
                    "agent_response": result.agent_response,
                    "token_usage": asdict(result.token_usage),
                    "processing_time": result.processing_time,
                    "rag_enabled": result.rag_enabled,
                    "examples_used": result.examples_used
                }
                for result in results
            ]
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ‰¹é‡ç»“æœå·²ä¿å­˜åˆ°: {output_file} å’Œ {json_file}")

def main():
    """ä¸»å‡½æ•°æ¼”ç¤º"""
    print("=== End-to-End NL2MTL Processing ===\n")
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = End2EndNL2MTL()
    
    # è®¾ç½®å¤„ç†æ¨¡å¼
    batch_mode = True  # è®¾ç½®ä¸ºTrueè¿›è¡Œæ‰¹é‡å¤„ç†ï¼ŒFalseè¿›è¡Œå•å¥å¤„ç†
    
    if not batch_mode:
        # å•å¥å¤„ç†ç¤ºä¾‹
        test_sentence = "Globally, if a is true, then b will be true in the next step."
        print(f"å¤„ç†å¥å­: {test_sentence}")
        print("-" * 60)
        
        result = processor.process_single(test_sentence)
        
        print(f"\nğŸ¤– Agentå›ç­”:")
        print(result.agent_response)
        print(f"\nğŸ“ æå–çš„MTLè¡¨è¾¾å¼: {result.final_mtl_expression}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
        print(f"ğŸ”¢ Tokenä½¿ç”¨: {result.token_usage.total_tokens}")
        print(f"ğŸ” RAGå¯ç”¨: {result.rag_enabled}")
        
        # ä¿å­˜ç»“æœ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = f"data/output/end2end/single_{timestamp}.json"
        processor.save_result(result, output_file)
        
    else:
        # æ‰¹é‡å¤„ç†ç¤ºä¾‹
        print("å¼€å§‹æ‰¹é‡å¤„ç†æ•°æ®é›†...")
        
        try:
            # è¯»å–æ•°æ®é›†
            dataset_df = pd.read_excel("data/input/nl2spec-dataset.xlsx")
            sentences = dataset_df["NL"].tolist()
            
            # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰
            # sentences = sentences[:10]  # åªå¤„ç†å‰10ä¸ª
            
            print(f"å…±åŠ è½½ {len(sentences)} ä¸ªå¥å­")
            
            # æ‰¹é‡å¤„ç†
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            output_file = f"data/output/end2end/batch_{timestamp}.xlsx"
            
            results = processor.process_batch(sentences, output_file)
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            total_tokens = sum(r.token_usage.total_tokens for r in results)
            total_time = sum(r.processing_time for r in results)
            successful_results = len([r for r in results if r.final_mtl_expression is not None])
            
            print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ!")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   - æ€»å¥å­æ•°: {len(results)}")
            print(f"   - æˆåŠŸæå–: {successful_results}")
            print(f"   - æˆåŠŸç‡: {successful_results/len(results)*100:.1f}%")
            print(f"   - æ€»Tokenä½¿ç”¨: {total_tokens}")
            print(f"   - æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
            print(f"   - å¹³å‡æ¯å¥ç”¨æ—¶: {total_time/len(results):.2f}ç§’")
            print(f"   - ç»“æœä¿å­˜è‡³: {output_file}")
            
        except FileNotFoundError as e:
            print(f"âŒ æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
            print("è¯·ç¡®ä¿ data/input/nl2spec-dataset.xlsx æ–‡ä»¶å­˜åœ¨")
        except Exception as e:
            print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")

if __name__ == "__main__":
    main()